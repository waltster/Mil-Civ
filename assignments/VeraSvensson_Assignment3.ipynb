{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803ea6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a88f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, weights, bias):\n",
    "    return np.dot(x, weights) + bias\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626bb93d",
   "metadata": {},
   "source": [
    "# Prepare training data\n",
    "Load csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20d04fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I stored the csv files in a folder called \"data\"\n",
    "xor_file = \"data/xor.csv\"\n",
    "or_file = \"data/or.csv\"\n",
    "and_file = \"data/and.csv\"\n",
    "nor_file = \"data/nor.csv\"\n",
    "nand_file = \"data/nand.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f205e44d",
   "metadata": {},
   "source": [
    "Read csv files using panda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3158c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_data = pd.read_csv(xor_file).values\n",
    "or_data = pd.read_csv(or_file).values\n",
    "and_data = pd.read_csv(and_file).values\n",
    "nor_data = pd.read_csv(nor_file).values\n",
    "nand_data = pd.read_csv(nand_file).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f16bc",
   "metadata": {},
   "source": [
    "Split into input and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46945767",
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_input = xor_data[:,0:2]\n",
    "xor_label = xor_data[:,2]\n",
    "or_input = or_data[:,0:2]\n",
    "or_label = or_data[:,2]\n",
    "and_input = and_data[:,0:2]\n",
    "and_label = and_data[:,2]\n",
    "nor_input = nor_data[:,0:2]\n",
    "nor_label = nor_data[:,2]\n",
    "nand_input = nand_data[:,0:2]\n",
    "nand_label = nand_data[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63916978",
   "metadata": {},
   "source": [
    "Combining into arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47696c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = [xor_input, or_input, and_input, nor_input, nand_input]\n",
    "training_labels = [xor_label, or_label, and_label, nor_label, nand_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9636dd66",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f4b23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch index in training_inputs and training_labels for current logic gate\n",
    "def class_index(logic_gate):\n",
    "    if logic_gate == \"xor\":\n",
    "        return 0\n",
    "    elif logic_gate == \"or\":\n",
    "        return 1\n",
    "    elif logic_gate == \"and\":\n",
    "        return 2\n",
    "    elif logic_gate == \"nor\":\n",
    "        return 3\n",
    "    elif logic_gate == \"nand\":\n",
    "        return 4\n",
    "    else:\n",
    "        # If all, or unvalid input, return -1, resulting in training on all gates\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a529091d",
   "metadata": {},
   "source": [
    "# Model\n",
    "Used the MLP code as reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21c5906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, num_features, num_hidden, num_output):\n",
    "        # Number of features\n",
    "        self.num_features = num_features \n",
    "        # Number of hidden layers\n",
    "        self.num_hidden = num_hidden\n",
    "        #Number of classes\n",
    "        \n",
    "        # Hidden layer weight matrix\n",
    "        # rows=neurons in hidden layer = num_hidden\n",
    "        # cols=neurons in prev layer = num_features\n",
    "        self.weights_hidden = np.zeros((num_hidden, num_features), dtype = float)\n",
    "        # Hidden layer bias\n",
    "        self.bias_hidden = np.zeros(num_hidden, dtype= float)\n",
    "        \n",
    "        # Output layer weight matrix\n",
    "        # rows=neurons in output = num_output\n",
    "        # cols=neurons in hidden layer = num_hidden\n",
    "        self.weights_output = np.zeros((num_output, num_hidden), dtype= float)\n",
    "        # Output bias\n",
    "        self.bias_output = np.zeros((num_output), dtype= float)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # Linear classifier on hidden layer\n",
    "        linear1 = linear(x, np.transpose(self.weights_hidden), self.bias_hidden)\n",
    "        # Wrapping output from hidden layer in sigmoid function\n",
    "        output1 = sigmoid(linear1)\n",
    "        \n",
    "        # Linear classifier on output from hidden layer\n",
    "        linear2 = linear(output1, np.transpose(self.weights_output), self.bias_output)\n",
    "        # Wrapping final output in sigmoid function\n",
    "        output2 = sigmoid(linear2)\n",
    "            \n",
    "        return output1, output2\n",
    "    \n",
    "    def mean_square_error(self, x, y):\n",
    "        # using the Mean-Squares method, implemented using numpy\n",
    "        \n",
    "        # output vs expected\n",
    "        diff =  np.subtract(y,x)\n",
    "        \n",
    "        # square difference and take the mean\n",
    "        mean_of_square = np.square(diff).mean()\n",
    "        \n",
    "        return mean_of_square\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        # Calculating the sigmoid derivative of x using method from MLP code\n",
    "        return x*(1-x)\n",
    "    \n",
    "    def backwards_propagation(self, x, out1, out2, y):\n",
    "        # Gradient for hidden to output?????????????????\n",
    "        loss_out = self.mean_square_error(out2,y) * self.sigmoid_derivative(out2)\n",
    "        # Gradient for weight, hidden to output layer\n",
    "        loss_out_w = np.dot(np.transpose(loss_out), out1)\n",
    "        # Gradient for bias, hidden to output layer\n",
    "        loss_out_b = np.sum(loss_out)\n",
    "       \n",
    "        \n",
    "        # Gradient for inout to hidden???????????????????\n",
    "        loss_hid = np.dot(loss_out, self.weights_output) * self.sigmoid_derivative(out1)\n",
    "         # Gradient for weight, input to hidden layer\n",
    "        loss_hid_w = np.dot(np.transpose(loss_hid), x)\n",
    "        # Gradient for bias, input to hidden layer\n",
    "        loss_hid_b = np.sum(loss_hid)\n",
    "        \n",
    "        return loss_out_w, loss_out_b, loss_hid_w, loss_hid_b\n",
    "    \n",
    "    def train(self, x, y, lr):\n",
    "        \n",
    "        #Predicting output 1 and 2\n",
    "        output1, output2 = self.predict(x)\n",
    "      \n",
    "        # Calculating errors\n",
    "        loss_out_w, loss_out_b, loss_hid_w, loss_hid_b = self.backwards_propagation(x, output1, output2, y)\n",
    "        \n",
    "        # == Update weights ==\n",
    "        #Adjusting weight for hidden layer\n",
    "        self.weights_hidden -= lr*loss_hid_w\n",
    "        #Adjusting bias for hidden layer\n",
    "        self.bias_hidden -= lr*loss_hid_b\n",
    "        #Adjusting weight for output\n",
    "        self.weights_output -= loss_out_w\n",
    "        #Adjusting bias for output\n",
    "        self.bias_output -= loss_out_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60589e",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07a06329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == Initiating model ==\n",
    "# num_features = 2. x1 and x2\n",
    "# num_hidden = 3. Feels like a good number when we only have 2 features\n",
    "# num_output = 1. Binary value between 0 and 1\n",
    "model = Model(num_features=2, num_hidden=3,num_output=1)\n",
    "\n",
    "# Learning rate = 0.01. Choosing 0.01 to start, could probably be optimized\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "# Number of times to run training loop. Choosing 10 since our data is so small\n",
    "TRAINING_ROUNDS = 10\n",
    "\n",
    "# What logic gate that will be trained. xor, or, and, nand, nor, all of them\n",
    "LOGIC_GATE = \"all\"\n",
    "# Fetching the index for the choosen logic gate\n",
    "INDEX = class_index(LOGIC_GATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e5a18",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "027852ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def run_training(index):\n",
    "    for training_rounds in range(TRAINING_ROUNDS):\n",
    "        model.train(training_inputs[index], training_labels[index], lr=LEARNING_RATE)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c5822a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if INDEX == -1:\n",
    "    # Train model with all gates\n",
    "    for gate in range(5):\n",
    "        run_training(gate)\n",
    "else:\n",
    "    # Train model with specified gate\n",
    "    run_training(INDEX)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
